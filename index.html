
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Let Me Show You: Learning by Retrieving from Video for Robotics</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta name="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="900">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://inner-monologue.github.io/"/>
    <meta property="og:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta property="og:description" content="Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta name="twitter:description" content="Project page for Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />
    <meta name="twitter:image" content="https://innermonologue.github.io/img/teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b>Let Me Show You: </br> Learning by Retrieving from Video for Robotics</br> 
                <!--<small>
                    
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
               <ul class="list-inline">
                <br>

    <li>Anonymous Author(s)</li>
        <br><br>
                
            </div>
        </div>

        

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                                
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Robots operating in complex and uncertain environments face considerable challenges. Advanced robotic systems often rely on extensive datasets to learn manipulation tasks. In contrast, when humans are faced with unfamiliar tasks, such as assembling a wooden chair, a common approach is to learn by watching video demonstrations. In this paper, we propose a novel method for learning robot policies by Retrieving-from-Video (RfV), using analogies from human demonstrations to address manipulation tasks. Our system constructs a video bank comprising recordings of humans performing diverse daily tasks. To enrich the knowledge from these videos, we extract mid-level information, such as object affordance masks and motion trajectories, which serve as additional inputs to enhance the robot model's learning and generalization capabilities. We further feature a dual-component system: a video retriever that taps into an external memory bank to fetch task-relevant video based on task specification, and a policy generator that integrates this retrieved knowledge into the learning cycle. This approach enables robots to craft adaptive responses to various scenarios and generalize to tasks beyond those in the training data. Through rigorous testing in multiple simulated and real-world settings, our system demonstrates a marked improvement in performance over conventional robotic systems, showcasing a significant breakthrough in the field of robotics.
                </p>

                <div class="text-center">
                    <video id="v0" width="90%" playsinline loop controls autoplay muted>
                        <source src="img/rfv_demo.mp4" type="video/mp4">
                        </video>
                </div>
            
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Framework
                </h3>
                <p class="text-justify">
                    In this paper, we present Retrieving-from-Video (RfV) which allows humans to show the robot how to complete manipulation tasks given specific instructions. Notably, plain human video with language description could contain high-level information, such as abstraction and reasoning of the scene, yet, this knowledge may not be beneficial to the robotic manipulation. To this end, we extract mid-level information, including object affordance and motion trajectory, that can be helpful for robots to learn low-level controls.

                    To realize our objectives for learning by retrieving, we introduce two modules, a video retriever and a policy generator. The video retriever module retrieves the task-relevant video from the video bank based on task specification. The policy generation module aims to effectively integrate mid-level information to facilitate both training and testing of the policy networks. 
                


            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments
                </h3>
		<p class="text-justify">
            We empirically assess the broad applicability of RfV across diverse tasks in both simulated and real-world settings. 
		</p>

        <h4>
            Experiments on simulator
        </h4>
         <p class="text-justify">
            Our method outperform multiple baselines on two simulated environment.
        </p>

        <div class="text-center">
                    <image src="img/rfv_sim.png" width="80%">
            </div>

        <h4>
            Experiments on real world
        </h4>
        <p class="text-justify">
            For the real robot experiments, our method outperform state-of-the-art robot foundation model. Also, we demonstrate the importance of using mid-level information (motion trajectory and object affordance).
        </p>

        <div class="text-center">
                <image src="img/rfv_real.png" width="80%">
            </div>

	    </div>
        </div>
            


</body>
</html>
